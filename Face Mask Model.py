# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ojfEUtnVi_r0uf9b-pzzUxi0EIxRckc6
"""

#Importing the necessary packages
from keras.optimizers import RMSprop
from keras.preprocessing.image import ImageDataGenerator
import cv2
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout
from keras.models import Model, load_model
from keras.callbacks import TensorBoard, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.utils import shuffle
from sklearn.metrics import precision_score

from imutils import paths
import numpy as np
import matplotlib
matplotlib.use
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import os

# Loading the list of images in our datasetthen initialize
#the list of data (i.e., images) and class images
#input te path of data set
Path = list(paths.list_images('/content/drive/My Drive/Dataset'))
data = []
labels = []
# loop over the image paths
for img in Path:
	# extract the class label from the filename
	label = img.split(os.path.sep)[-2]
	# load the input image (150x150) and preprocess it
	image = load_img(img, target_size=(150, 150))
	image = img_to_array(image)
	image = preprocess_input(image)
	# update the data and labels lists, respectively
	data.append(image)
	labels.append(label)
# convert the data and labels to NumPy arrays
data = np.array(data, dtype="float32")
labels = np.array(labels)

"""Splitting the Data"""

# partition the data into training and testing splits using 80% of
# the data for training and the remaining 20% for testing
(x_train, x_test, y_train, y_test) = train_test_split(data, labels,
	test_size=0.20, random_state=42)

"""Creating the Network"""

model =Sequential([

    Conv2D(100, (3,3), activation='relu', 

    input_shape=(150, 150, 3)),

    MaxPooling2D(2,2),

    Conv2D(100, (3,3), activation='relu'),

    MaxPooling2D(2,2),

    Flatten(),

    Dropout(0.5),

    Dense(50, activation='relu'),

    Dense(2, activation='softmax')

])
model.summary()
print("compiling the model")
model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])

"""Image Data Generation/Augmentation"""

#constructing the training image generator for data augmentation
#to increase the size of data set
#TRAINING_DIR takes the path of train dataset
TRAINING_DIR = '/content/drive/My Drive/Dataset/train'
train_datagen = ImageDataGenerator(rescale=1.0/255,
                                   rotation_range=40,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   fill_mode='nearest')

train_generator = train_datagen.flow_from_directory(TRAINING_DIR, 
                                                    batch_size=32, 
                                                    target_size=(150, 150))
#constructing the validation image generator for data augmentation to perofrm on test data
#VALIDATION_DIR takes the path of test dataset
VALIDATION_DIR = '/content/drive/My Drive/Dataset/test'
validation_datagen = ImageDataGenerator(rescale=1.0/255)

validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, 
                                                         batch_size=128, 
                                                         target_size=(150, 150))

"""Training the Model"""

print("[INFO] training head...")
H = model.fit_generator(train_generator,
                              epochs=20,
                              validation_data=validation_generator,
                              callbacks=[checkpoint])

"""Saving the model"""

#Initializing a callback checkpoint to keep saving best model after each epoch while training:
checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')

"""Plotting the Graph"""

N = 20
#using ggplot style
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["acc"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")